{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentflow.env import VecGymEnv\n",
    "from agentflow.agents import DDPG\n",
    "from agentflow.buffers import PrioritizedBufferMap\n",
    "from agentflow.state import NPrevFramesStateEnv\n",
    "from agentflow.tensorflow.nn import dense_net\n",
    "from agentflow.tensorflow.ops import normalize_ema\n",
    "from agentflow.utils import check_whats_connected\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "discrete = True\n",
    "dqda_clipping = 1\n",
    "clip_norm = True\n",
    "hidden_dims = 32\n",
    "hidden_layers = 2\n",
    "output_dim = 2\n",
    "batchnorm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Environment & Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_net(x,units,layers,batchnorm=True,activation=tf.nn.relu,training=False,**kwargs):\n",
    "\n",
    "    assert isinstance(layers,int) and layers > 0, 'layers should be a positive integer'\n",
    "    assert isinstance(units,int) and units > 0, 'units should be a positive integer'\n",
    "\n",
    "    h = x\n",
    "    for l in range(layers):\n",
    "        h = tf.layers.dense(h,units,**kwargs)\n",
    "        h = activation(h)\n",
    "        \n",
    "        if batchnorm:\n",
    "            BN = tf.layers.BatchNormalization()\n",
    "            h = BN(h,training=training)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net_fn(hidden_dims,hidden_layers,output_dim,batchnorm):\n",
    "    def net_fn(state,training=False):\n",
    "        h = state\n",
    "        if batchnorm:\n",
    "            BN = tf.layers.BatchNormalization()\n",
    "            h = BN(h,training=training)\n",
    "        \n",
    "            #with tf.control_dependencies(BN.updates):\n",
    "            h = dense_net(h,hidden_dims,hidden_layers,batchnorm=False,training=training)\n",
    "        else:\n",
    "            h = dense_net(h,hidden_dims,hidden_layers,batchnorm=False,training=training)\n",
    "            \n",
    "        return tf.layers.dense(h,output_dim)\n",
    "    return net_fn\n",
    "\n",
    "def build_policy_fn(*args,**kwargs):\n",
    "    net_fn = build_net_fn(*args,**kwargs)\n",
    "    def policy_fn(state,training=False):\n",
    "        h = net_fn(state)\n",
    "        return tf.nn.softmax(h,axis=-1)\n",
    "    return policy_fn\n",
    "\n",
    "def build_q_fn(*args,**kwargs):\n",
    "    net_fn = build_net_fn(*args,**kwargs)\n",
    "    def q_fn(state,action,training=False):\n",
    "        h = tf.concat([state,action],axis=1)\n",
    "        return net_fn(h)\n",
    "    return q_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 16), (10,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = VecGymEnv(env_id,n_envs=10)\n",
    "env = NPrevFramesStateEnv(env,n_prev_frames=4,flatten=True)\n",
    "\n",
    "test_env = VecGymEnv(env_id,n_envs=1)\n",
    "test_env = NPrevFramesStateEnv(test_env,n_prev_frames=4,flatten=True)\n",
    "\n",
    "state = env.reset()\n",
    "state_shape = state.shape\n",
    "action_shape = env.env.action_shape() \n",
    "state_shape, action_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_fn = build_policy_fn(hidden_dims,hidden_layers,output_dim,batchnorm)\n",
    "q_fn = build_q_fn(hidden_dims,hidden_layers,1,batchnorm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1227 08:44:59.949558 4720461248 deprecation_wrapper.py:119] From /Users/justinmaojones/rl/agentflow/agents/ddpg.py:43: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1227 08:44:59.966495 4720461248 deprecation_wrapper.py:119] From /Users/justinmaojones/rl/agentflow/agents/ddpg.py:59: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1227 08:45:00.038328 4720461248 deprecation.py:323] From <ipython-input-4-b7b951101ec6>:8: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1227 08:45:00.040128 4720461248 deprecation.py:506] From /Users/justinmaojones/rl/env/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1227 08:45:00.348133 4720461248 deprecation_wrapper.py:119] From /Users/justinmaojones/rl/agentflow/tensorflow/ops.py:5: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "W1227 08:45:00.349045 4720461248 deprecation.py:323] From /Users/justinmaojones/rl/env/lib/python3.7/site-packages/tensorflow/python/training/moving_averages.py:433: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W1227 08:45:00.652474 4720461248 deprecation.py:323] From /Users/justinmaojones/rl/env/lib/python3.7/site-packages/tensorflow/python/ops/clip_ops.py:157: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1227 08:45:00.662465 4720461248 deprecation_wrapper.py:119] From /Users/justinmaojones/rl/agentflow/agents/ddpg.py:102: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "W1227 08:45:00.663079 4720461248 deprecation_wrapper.py:119] From /Users/justinmaojones/rl/agentflow/agents/ddpg.py:103: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "W1227 08:45:00.663992 4720461248 deprecation_wrapper.py:119] From /Users/justinmaojones/rl/agentflow/agents/ddpg.py:103: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "W1227 08:45:00.910654 4720461248 deprecation.py:506] From /Users/justinmaojones/rl/env/lib/python3.7/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "agent = DDPG(state_shape[1:],[2],policy_fn,q_fn,dqda_clipping,clip_norm,discrete=discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(test_env,agent):\n",
    "    state, rt, done = test_env.reset(), 0, 0\n",
    "    while np.sum(done) == 0:\n",
    "        action = agent.act(state).argmax(axis=-1).ravel()\n",
    "        state, reward, done, _ = test_env.step(action)\n",
    "        rt += reward.sum()\n",
    "    return rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1./(1.+np.exp(-x))\n",
    "\n",
    "def invert_softmax(x,axis=-1):\n",
    "    return np.log(x)\n",
    "\n",
    "def softmax(x,axis=-1):\n",
    "    return np.exp(x)/np.exp(x).sum(axis=axis,keepdims=True)\n",
    "\n",
    "def onehot(x,depth=2):\n",
    "    shape = list(x.shape)+[2]\n",
    "    y = np.zeros(shape)\n",
    "    y[np.arange(len(x)),x] = 1.\n",
    "    return y.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noisy_action(action,eps=1.,clip=5e-2):\n",
    "    action = np.minimum(1-clip,np.maximum(clip,action))\n",
    "    logit = invert_softmax(action)\n",
    "    u = np.random.rand(*action.shape)\n",
    "    g = -np.log(-np.log(u))\n",
    "    return (eps*g+logit).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0443"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.concatenate([np.ones((10000,1)),np.zeros((10000,1))],axis=-1)\n",
    "noisy_action(action,eps=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9507"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = np.concatenate([np.zeros((10000,1)),np.ones((10000,1))],axis=-1)\n",
    "noisy_action(action,eps=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARVALS = {v.name:tf.reduce_mean(tf.square(v)) for v in tf.global_variables()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function numpy.ones_like(a, dtype=None, order='K', subok=True)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10044/20000 [==============>...............] - ETA: 7:33 - avg_action: 0.4891 - test_ep_returns: 271.0200"
     ]
    }
   ],
   "source": [
    "replay_buffer = PrioritizedBufferMap(2**11,alpha=0.5,eps=0.1)\n",
    "\n",
    "reward_history = []\n",
    "action_history = []\n",
    "variable_vals = []\n",
    "test_ep_returns = []\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    B = int(2e3)\n",
    "    T = int(2e4)\n",
    "    batchsize = 100\n",
    "    pb = tf.keras.utils.Progbar(T)\n",
    "    for t in range(T):\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        if len(replay_buffer) > B:\n",
    "            action = noisy_action(action)\n",
    "        else:\n",
    "            action = np.random.choice(2,size=len(action))\n",
    "        \n",
    "        state2, reward, done, info = env.step(action.astype('int').ravel())\n",
    "        \n",
    "        reward_history.append(reward)\n",
    "        action_history.append(action)\n",
    "        \n",
    "        replay_buffer.append(\n",
    "            {'state':state,'action':onehot(action),'reward':reward,'done':done,'state2':state2},\n",
    "            priority = np.ones_like(reward)\n",
    "        )\n",
    "        state = state2\n",
    "        \n",
    "        if len(replay_buffer) >= B:\n",
    "            for i in range(40):\n",
    "                beta = t*1./T\n",
    "                td_error = agent.update(learning_rate=1e-4,**replay_buffer.sample(batchsize,beta=beta))\n",
    "                replay_buffer.update_priorities(td_error)\n",
    "                \n",
    "        #variable_vals.append(sess.run(VARVALS))\n",
    "            \n",
    "        if t % 100 == 0 and t > 0:\n",
    "            test_ep_returns.append(test_agent(test_env,agent))\n",
    "            \n",
    "            pb.add(1,[('avg_action', action.mean()),('test_ep_returns', test_ep_returns[-1])])\n",
    "        else:\n",
    "            \n",
    "            pb.add(1,[('avg_action', action.mean())])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history_arr = np.stack(reward_history)\n",
    "action_history_arr = np.stack(action_history)\n",
    "variable_vals_df = pd.DataFrame(variable_vals)\n",
    "test_ep_returns_arr = np.stack(test_ep_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(100*np.arange(len(test_ep_returns_arr)),test_ep_returns_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = replay_buffer._sum_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipr = 1./pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.log2(pr.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.log2(ipr.ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipr_sum = replay_buffer._inv_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = replay_buffer._sum_tree.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ipr / ipr_sum).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = ipr/ipr_sum*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.log2(w.ravel()),normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(np.log2(ipr.ravel()),normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_history_arr.mean(axis=1))\n",
    "plt.plot(reward_history_arr.max(axis=1),alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action_history_arr.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for c in sorted(variable_vals_df.columns):\n",
    "    variable_vals_df[c].apply(np.log).plot(title=c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
